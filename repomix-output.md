This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-02-06T05:29:28.237Z

# File Summary

## Purpose
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A header with the file path (## File: path/to/file)
  b. The full contents of the file in a code block

## Usage Guidelines
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

## Notes
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

## Additional Info

# Directory Structure
```
.github/
  workflows/
    deploy.yml
backend/
  docubot_agent/
    .env.sample
    main.py
  src/
    main.py
    requirements.txt
  Dockerfile
  README.md
frontend/
  src/
    app/
      chat/
        ChatUI.tsx
        page.tsx
      globals.css
      layout.tsx
      page.tsx
    public/
      next.svg
      vercel.svg
  .eslintrc.json
  Dockerfile
  next.config.js
  package.json
  postcss.config.js
  tailwind.config.js
  tsconfig.json
terraform/
  app/
    main.tf
    variables.tf
  init/
    main.tf
    variables.tf
.env_sample
.gitignore
.windsurfrules
compose-init.yaml
compose.yaml
Makefile
README.md
```

# Files

## File: .github/workflows/deploy.yml
````yaml
name: Deploy to Cloud Run

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main

env:
  PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
  REGION: ${{ secrets.GCP_REGION }}
  ARTIFACT_REPO: ${{ secrets.ARTIFACT_REPO }}

jobs:
  deploy:
    permissions:
      contents: 'read'
      id-token: 'write'

    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - id: auth
        uses: google-github-actions/auth@v2
        with:
          workload_identity_provider: ${{ secrets.WORKLOAD_IDENTITY_PROVIDER }}
          service_account: ${{ secrets.BUILD_ACCOUNT }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2

      - name: Configure docker for artifact registry
        run: |
          gcloud auth configure-docker ${{ env.REGION }}-docker.pkg.dev

      - name: Build and push backend image
        run: |
          docker build -t ${{ env.REGION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.ARTIFACT_REPO }}/backend:${{ github.sha }} ./backend
          docker push ${{ env.REGION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.ARTIFACT_REPO }}/backend:${{ github.sha }}

      - name: Build and push frontend image
        run: |
          docker build -t ${{ env.REGION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.ARTIFACT_REPO }}/frontend:${{ github.sha }} ./frontend
          docker push ${{ env.REGION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.ARTIFACT_REPO }}/frontend:${{ github.sha }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.5.7"

      - name: Terraform Init
        run: |
          cd terraform/app
          terraform init \
            -backend-config="bucket=${{ env.PROJECT_ID }}-tfstate" \
            -backend-config="prefix=terraform/state"

      - name: Terraform Plan
        run: |
          cd terraform/app
          terraform plan \
            -var="project_id=${{ env.PROJECT_ID }}" \
            -var="region=${{ env.REGION }}" \
            -var="backend_image=${{ env.REGION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.ARTIFACT_REPO }}/backend:${{ github.sha }}" \
            -var="frontend_image=${{ env.REGION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.ARTIFACT_REPO }}/frontend:${{ github.sha }}"

      - name: Terraform Apply
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        run: |
          cd terraform/app
          terraform apply -auto-approve \
            -var="project_id=${{ env.PROJECT_ID }}" \
            -var="region=${{ env.REGION }}" \
            -var="backend_image=${{ env.REGION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.ARTIFACT_REPO }}/backend:${{ github.sha }}" \
            -var="frontend_image=${{ env.REGION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.ARTIFACT_REPO }}/frontend:${{ github.sha }}"
````

## File: backend/docubot_agent/.env.sample
````
# OpenAI API Key
OPENAI_API_KEY=your_api_key_here
LANGCHAIN_TRACING_V2=
LANGCHAIN_ENDPOINT=
LANGCHAIN_API_KEY=
LANGCHAIN_PROJECT=
````

## File: backend/docubot_agent/main.py
````python
import operator
from typing import Annotated, Any, Optional
import os
from dotenv import load_dotenv
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langgraph.graph import END, StateGraph
from pydantic import BaseModel, Field

# .envファイルから環境変数を読み込む
load_dotenv()


# ペルソナを表すデータモデル
class Persona(BaseModel):
    name: str = Field(..., description="ペルソナの名前")
    background: str = Field(..., description="ペルソナの持つ背景")


# ペルソナのリストを表すデータモデル
class Personas(BaseModel):
    personas: list[Persona] = Field(
        default_factory=list, description="ペルソナのリスト"
    )


# インタビュー内容を表すデータモデル
class Interview(BaseModel):
    persona: Persona = Field(..., description="インタビュー対象のペルソナ")
    question: str = Field(..., description="インタビューでの質問")
    answer: str = Field(..., description="インタビューでの回答")


# インタビュー結果のリストを表すデータモデル
class InterviewResult(BaseModel):
    interviews: list[Interview] = Field(
        default_factory=list, description="インタビュー結果のリスト"
    )


# 評価の結果を表すデータモデル
class EvaluationResult(BaseModel):
    reason: str = Field(..., description="判断の理由")
    is_sufficient: bool = Field(..., description="情報が十分かどうか")


# 要件定義生成AIエージェントのステート
class InterviewState(BaseModel):
    user_request: str = Field(..., description="ユーザーからのリクエスト")
    personas: Annotated[list[Persona], operator.add] = Field(
        default_factory=list, description="生成されたペルソナのリスト"
    )
    interviews: Annotated[list[Interview], operator.add] = Field(
        default_factory=list, description="実施されたインタビューのリスト"
    )
    requirements_doc: str = Field(default="", description="生成された要件定義")
    iteration: int = Field(
        default=0, description="ペルソナ生成とインタビューの反復回数"
    )
    is_information_sufficient: bool = Field(
        default=False, description="情報が十分かどうか"
    )


# ペルソナを生成するクラス
class PersonaGenerator:
    def __init__(self, llm: ChatOpenAI, k: int = 5):
        self.llm = llm.with_structured_output(Personas)
        self.k = k

    def run(self, user_request: str) -> Personas:
        # プロンプトテンプレートを定義
        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    "あなたはユーザーインタビュー用の多様なペルソナを作成する専門家です。",
                ),
                (
                    "human",
                    f"以下のユーザーリクエストに関するインタビュー用に、{self.k}人の多様なペルソナを生成してください。\n\n"
                    "ユーザーリクエスト: {user_request}\n\n"
                    "各ペルソナには名前と簡単な背景を含めてください。年齢、性別、職業、技術的専門知識において多様性を確保してください。",
                ),
            ]
        )
        # ペルソナ生成のためのチェーンを作成
        chain = prompt | self.llm
        # ペルソナを生成
        return chain.invoke({"user_request": user_request})


# インタビューを実施するクラス
class InterviewConductor:
    def __init__(self, llm: ChatOpenAI):
        self.llm = llm

    def run(self, user_request: str, personas: list[Persona]) -> InterviewResult:
        # 質問を生成
        questions: list[str] = self._generate_questions(
            user_request=user_request, personas=personas
        )
        # 回答を生成
        answers: list[str] = self._generate_answers(
            personas=personas, questions=questions
        )
        # 質問と回答の組み合わせからインタビューリストを作成
        interviews: list[Interview] = self._create_interviews(
            personas=personas, questions=questions, answers=answers
        )
        # インタビュー結果を返す
        return InterviewResult(interviews=interviews)

    def _generate_questions(
        self, user_request: str, personas: list[Persona]
    ) -> list[str]:
        # 質問生成のためのプロンプトを定義
        question_prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    "あなたはユーザー要件に基づいて適切な質問を生成する専門家です。",
                ),
                (
                    "human",
                    "以下のペルソナに関連するユーザーリクエストについて、1つの質問を生成してください。\n\n"
                    "ユーザーリクエスト: {user_request}\n"
                    "ペルソナ: {persona_name} - {persona_background}\n\n"
                    "質問は具体的で、このペルソナの視点から重要な情報を引き出すように設計してください。",
                ),
            ]
        )
        # 質問生成のためのチェーンを作成
        question_chain = question_prompt | self.llm | StrOutputParser()

        # 各ペルソナに対する質問クエリを作成
        question_queries = [
            {
                "user_request": user_request,
                "persona_name": persona.name,
                "persona_background": persona.background,
            }
            for persona in personas
        ]
        # 質問をバッチ処理で生成
        return question_chain.batch(question_queries)

    def _generate_answers(
        self, personas: list[Persona], questions: list[str]
    ) -> list[str]:
        # 回答生成のためのプロンプトを定義
        answer_prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    "あなたは以下のペルソナとして回答しています: {persona_name} - {persona_background}",
                ),
                ("human", "質問: {question}"),
            ]
        )
        # 回答生成のためのチェーンを作成
        answer_chain = answer_prompt | self.llm | StrOutputParser()

        # 各ペルソナに対する回答クエリを作成
        answer_queries = [
            {
                "persona_name": persona.name,
                "persona_background": persona.background,
                "question": question,
            }
            for persona, question in zip(personas, questions)
        ]
        # 回答をバッチ処理で生成
        return answer_chain.batch(answer_queries)

    def _create_interviews(
        self, personas: list[Persona], questions: list[str], answers: list[str]
    ) -> list[Interview]:
        # ペルソナ毎に質問と回答の組み合わせからインタビューオブジェクトを作成
        return [
            Interview(persona=persona, question=question, answer=answer)
            for persona, question, answer in zip(personas, questions, answers)
        ]


# 情報の十分性を評価するクラス
class InformationEvaluator:
    def __init__(self, llm: ChatOpenAI):
        self.llm = llm.with_structured_output(EvaluationResult)

    # ユーザーリクエストとインタビュー結果を基に情報の十分性を評価
    def run(self, user_request: str, interviews: list[Interview]) -> EvaluationResult:
        # プロンプトを定義
        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    "あなたは包括的な要件文書を作成するための情報の十分性を評価する専門家です。",
                ),
                (
                    "human",
                    "以下のユーザーリクエストとインタビュー結果に基づいて、包括的な要件文書を作成するのに十分な情報が集まったかどうかを判断してください。\n\n"
                    "ユーザーリクエスト: {user_request}\n\n"
                    "インタビュー結果:\n{interview_results}",
                ),
            ]
        )
        # 情報の十分性を評価するチェーンを作成
        chain = prompt | self.llm
        # 評価結果を返す
        return chain.invoke(
            {
                "user_request": user_request,
                "interview_results": "\n".join(
                    f"ペルソナ: {i.persona.name} - {i.persona.background}\n"
                    f"質問: {i.question}\n回答: {i.answer}\n"
                    for i in interviews
                ),
            }
        )


# 要件定義書を生成するクラス
class RequirementsDocumentGenerator:
    def __init__(self, llm: ChatOpenAI):
        self.llm = llm

    def run(self, user_request: str, interviews: list[Interview]) -> str:
        # プロンプトを定義
        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    "あなたは収集した情報に基づいて要件文書を作成する専門家です。",
                ),
                (
                    "human",
                    "以下のユーザーリクエストと複数のペルソナからのインタビュー結果に基づいて、要件文書を作成してください。\n\n"
                    "ユーザーリクエスト: {user_request}\n\n"
                    "インタビュー結果:\n{interview_results}\n"
                    "要件文書には以下のセクションを含めてください:\n"
                    "1. プロジェクト概要\n"
                    "2. 主要機能\n"
                    "3. 非機能要件\n"
                    "4. 制約条件\n"
                    "5. ターゲットユーザー\n"
                    "6. 優先順位\n"
                    "7. リスクと軽減策\n\n"
                    "出力は必ず日本語でお願いします。\n\n要件文書:",
                ),
            ]
        )
        # 要件定義書を生成するチェーンを作成
        chain = prompt | self.llm | StrOutputParser()
        # 要件定義書を生成
        return chain.invoke(
            {
                "user_request": user_request,
                "interview_results": "\n".join(
                    f"ペルソナ: {i.persona.name} - {i.persona.background}\n"
                    f"質問: {i.question}\n回答: {i.answer}\n"
                    for i in interviews
                ),
            }
        )


# 要件定義書生成AIエージェントのクラス
class DocumentationAgent:
    def __init__(self, llm: ChatOpenAI, k: Optional[int] = None):
        # 各種ジェネレータの初期化
        self.persona_generator = PersonaGenerator(llm=llm, k=k)
        self.interview_conductor = InterviewConductor(llm=llm)
        self.information_evaluator = InformationEvaluator(llm=llm)
        self.requirements_generator = RequirementsDocumentGenerator(llm=llm)

        # グラフの作成
        self.graph = self._create_graph()

    def _create_graph(self) -> StateGraph:
        # グラフの初期化
        workflow = StateGraph(InterviewState)

        # 各ノードの追加
        workflow.add_node("generate_personas", self._generate_personas)
        workflow.add_node("conduct_interviews", self._conduct_interviews)
        workflow.add_node("evaluate_information", self._evaluate_information)
        workflow.add_node("generate_requirements", self._generate_requirements)

        # エントリーポイントの設定
        workflow.set_entry_point("generate_personas")

        # ノード間のエッジの追加
        workflow.add_edge("generate_personas", "conduct_interviews")
        workflow.add_edge("conduct_interviews", "evaluate_information")

        # 条件付きエッジの追加
        workflow.add_conditional_edges(
            "evaluate_information",
            lambda state: not state.is_information_sufficient and state.iteration < 5,
            {True: "generate_personas", False: "generate_requirements"},
        )
        workflow.add_edge("generate_requirements", END)

        # グラフのコンパイル
        return workflow.compile()

    def _generate_personas(self, state: InterviewState) -> dict[str, Any]:
        # ペルソナの生成
        new_personas: Personas = self.persona_generator.run(state.user_request)
        return {
            "personas": new_personas.personas,
            "iteration": state.iteration + 1,
        }

    def _conduct_interviews(self, state: InterviewState) -> dict[str, Any]:
        # インタビューの実施
        new_interviews: InterviewResult = self.interview_conductor.run(
            state.user_request, state.personas[-5:]
        )
        return {"interviews": new_interviews.interviews}

    def _evaluate_information(self, state: InterviewState) -> dict[str, Any]:
        # 情報の評価
        evaluation_result: EvaluationResult = self.information_evaluator.run(
            state.user_request, state.interviews
        )
        return {
            "is_information_sufficient": evaluation_result.is_sufficient,
            "evaluation_reason": evaluation_result.reason,
        }

    def _generate_requirements(self, state: InterviewState) -> dict[str, Any]:
        # 要件定義書の生成
        requirements_doc: str = self.requirements_generator.run(
            state.user_request, state.interviews
        )
        return {"requirements_doc": requirements_doc}

    def run(self, user_request: str) -> str:
        # 初期状態の設定
        initial_state = InterviewState(user_request=user_request)
        # グラフの実行
        final_state = self.graph.invoke(initial_state)
        # 最終的な要件定義書の取得
        return final_state["requirements_doc"]


# 実行方法:
# poetry run python -m documentation_agent.main --task "ユーザーリクエストをここに入力してください"
# 実行例）
# poetry run python -m documentation_agent.main --task "スマートフォン向けの健康管理アプリを開発したい"
def main():
    import argparse

    # コマンドライン引数のパーサーを作成
    parser = argparse.ArgumentParser(
        description="ユーザー要求に基づいて要件定義を生成します"
    )
    # "task"引数を追加
    parser.add_argument(
        "--task",
        type=str,
        help="作成したいアプリケーションについて記載してください",
    )
    # "k"引数を追加
    parser.add_argument(
        "--k",
        type=int,
        default=5,
        help="生成するペルソナの人数を設定してください（デフォルト:5）",
    )
    # コマンドライン引数を解析
    args = parser.parse_args()

    # ChatOpenAIモデルを初期化（deepseek-chatを使用）
    llm = ChatOpenAI(
        model="gpt-4o",
        openai_api_key=os.getenv("OPENAI_API_KEY"),
    )
    # 要件定義書生成AIエージェントを初期化
    agent = DocumentationAgent(llm=llm, k=args.k)
    # エージェントを実行して最終的な出力を取得
    final_output = agent.run(user_request=args.task)

    # 最終的な出力を表示
    print(final_output)


if __name__ == "__main__":
    main()
````

## File: backend/src/main.py
````python
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import sys
import os
from dotenv import load_dotenv
from api.docubot_agent.main import DocumentationAgent
from langchain_openai import ChatOpenAI

# .envファイルを読み込む
load_dotenv()

# docubot_agentモジュールをインポートできるようにパスを追加
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

app = FastAPI(title="DocuBot API", version="1.0.0")

# CORS設定
origins = [
    "http://localhost:3000",
    "https://documentbot-agent.vercel.app",
    "https://documentbot-agent-git-main-toshiyan76.vercel.app",
    "https://documentbot-agent-toshiyan76.vercel.app"
]

# 環境変数からCORSの追加設定を取得
if additional_origins := os.getenv("CORS_ORIGINS"):
    origins.extend(additional_origins.split(","))

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    message: str

# エージェント初期化
llm = ChatOpenAI()
agent = DocumentationAgent(llm=llm)

@app.get("/health")
async def health_check():
    """
    ヘルスチェックエンドポイント
    サーバーの状態を確認するために使用
    """
    return {
        "status": "healthy",
        "version": "1.0.0",
        "openai_api_key": bool(os.getenv("OPENAI_API_KEY"))
    }

@app.post("/api/chat")
async def chat_endpoint(request: ChatRequest):
    """
    チャットエンドポイント
    ユーザーからのメッセージを受け取り、AIエージェントの応答を返す
    """
    try:
        # デバッグログ
        print(f"Received message: {request.message}")
        print(f"OPENAI_API_KEY is set: {bool(os.getenv('OPENAI_API_KEY'))}")
        
        # エージェント処理を呼び出し
        response = agent.run(request.message)
        return {"response": response}
    except Exception as e:
        import traceback
        error_detail = {
            "error": str(e),
            "traceback": traceback.format_exc()
        }
        print("Error occurred:", error_detail)
        raise HTTPException(status_code=500, detail=error_detail)

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", "8080"))
    uvicorn.run(app, host="0.0.0.0", port=port, debug=True)
````

## File: backend/src/requirements.txt
````
fastapi==0.109.0
uvicorn[standard]==0.27.0
python-dotenv==1.0.0
````

## File: backend/Dockerfile
````
# syntax=docker/dockerfile:1
FROM python:3.10-slim as base

WORKDIR /app
ENV PYTHONPATH=/app

# 開発環境用
FROM base as init
COPY src/requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8080", "--reload"]

# 本番環境用
FROM base as runner
COPY src/ .
RUN pip install --no-cache-dir -r requirements.txt
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8080"]
````

## File: backend/README.md
````markdown
# バックエンドテスト手順書

このドキュメントでは、DocuBotのバックエンドAPIをテストする手順を説明します。

## 1. Docker環境でのテスト（推奨）

### 1.1 Dockerコンテナの起動

1. プロジェクトのルートディレクトリに移動
   ```bash
   cd /path/to/docubot-agent
   ```

2. 環境変数の設定
   ```bash
   cp .env.sample .env
   # .envファイルを編集してOpenAI APIキーを設定
   ```

3. Dockerコンテナのビルドと起動
   ```bash
   docker-compose up --build backend
   ```

### 1.2 APIの動作確認

1. ヘルスチェック
   ```bash
   curl http://localhost:8000/health
   ```
   期待される応答：
   ```json
   {
     "status": "healthy",
     "version": "1.0.0",
     "openai_api_key": true
   }
   ```

2. チャットエンドポイントのテスト
   ```bash
   curl -X POST "http://localhost:8000/chat" \
   -H "Content-Type: application/json" \
   -d '{"message": "新しいECサイトの要件を定義したい"}'
   ```

3. Swagger UIの確認
   ブラウザで以下のURLにアクセス：
   ```
   http://localhost:8000/docs
   ```

### 1.3 コンテナログの確認
```bash
docker-compose logs -f backend
```

### 1.4 トラブルシューティング

1. コンテナの状態確認
   ```bash
   docker-compose ps
   ```

2. コンテナの再起動
   ```bash
   docker-compose restart backend
   ```

3. コンテナの再ビルド
   ```bash
   docker-compose build --no-cache backend
   ```

4. 依存関係の更新
   ```bash
   # コンテナ内で実行
   docker-compose exec backend uv sync
   ```

## 2. 手動テスト（基本）

### 2.1 開発サーバーの起動

1. バックエンドディレクトリに移動
   ```bash
   cd backend
   ```

2. 依存関係のインストール
   ```bash
   uv pip install --system
   # 開発時は
   uv pip install --system -G dev
   ```

3. 開発サーバーの起動
   ```bash
   uvicorn main:app --reload
   ```

### 2.2 APIドキュメントの確認

1. ブラウザで以下のURLにアクセス：
   ```
   http://localhost:8000/docs
   ```
   Swaggerドキュメントが表示され、利用可能なAPIエンドポイントが確認できます。

### 2.3 基本的なAPIテスト

1. curlを使用したテスト
   ```bash
   # チャットエンドポイントのテスト
   curl -X POST "http://localhost:8000/chat" \
   -H "Content-Type: application/json" \
   -d '{"message": "新しいECサイトの要件を定義したい"}'
   ```

2. HTTPieを使用したテスト（より読みやすい）
   ```bash
   # HTTPieのインストール
   pip install httpie

   # テストの実行
   http POST http://localhost:8000/chat message="新しいECサイトの要件を定義したい"
   ```

## 3. 自動テストの実行（発展）

### 3.1 テスト環境のセットアップ

1. テスト用パッケージのインストール
   ```bash
   pip install pytest pytest-asyncio httpx
   ```

2. テストファイルの作成
   ```bash
   mkdir tests
   touch tests/__init__.py
   touch tests/test_main.py
   ```

3. `tests/test_main.py`に以下のテストコードを追加：
   ```python
   import pytest
   from fastapi.testclient import TestClient
   from main import app

   client = TestClient(app)

   def test_chat_endpoint():
       response = client.post(
           "/chat",
           json={"message": "新しいECサイトの要件を定義したい"}
       )
       assert response.status_code == 200
       assert "response" in response.json()
   ```

4. テストの実行
   ```bash
   pytest
   ```

## 4. よくあるエラーと解決方法

### 4.1 サーバー起動時のエラー

1. ポートが使用中の場合
   ```bash
   # 別のポートを指定して起動
   uvicorn main:app --reload --port 8001
   ```

2. モジュールが見つからない場合
   ```bash
   # 依存関係を再インストール
   pip install -r requirements.txt
   ```

### 4.2 API呼び出し時のエラー

1. CORS関連のエラー
   - フロントエンドからのアクセスがブロックされる場合は、`main.py`のCORS設定を確認

2. OpenAI APIキーのエラー
   - `.env`ファイルにAPIキーが正しく設定されているか確認
   - 環境変数が読み込まれているか確認
   ```bash
   echo $OPENAI_API_KEY
   ```

## 5. パフォーマンステスト（オプション）

### 5.1 負荷テストの実行

1. locustのインストール
   ```bash
   pip install locust
   ```

2. `locustfile.py`の作成
   ```python
   from locust import HttpUser, task, between

   class APIUser(HttpUser):
       wait_time = between(1, 2)

       @task
       def test_chat(self):
           self.client.post("/chat", json={
               "message": "新しいECサイトの要件を定義したい"
           })
   ```

3. 負荷テストの実行
   ```bash
   locust -f locustfile.py
   ```

4. ブラウザで`http://localhost:8089`にアクセスし、テストを設定・実行

## 6. テスト結果の解釈

### 6.1 期待される結果

1. APIエンドポイント（/chat）
   - ステータスコード: 200
   - レスポンス形式: JSON
   - 必須フィールド: "response"

2. エラーレスポンス
   - 不正なリクエスト: 400
   - サーバーエラー: 500
   - エラーメッセージを含むJSON応答

### 6.2 パフォーマンス指標

- レスポンス時間: 通常2-5秒以内
- エラー率: 1%未満
- 同時接続数: 最大10接続まで安定動作

## 7. トラブルシューティング

問題が発生した場合は、以下の手順で対応してください：

1. ログの確認
   ```bash
   uvicorn main:app --reload --log-level debug
   ```

2. 環境変数の確認
   ```bash
   python -c "import os; print(os.environ.get('OPENAI_API_KEY'))"
   ```

3. ネットワーク接続の確認
   ```bash
   curl -v http://localhost:8000/health
   ```

4. 仮想環境の再作成
   ```bash
   deactivate
   rm -rf .venv
   python -m venv .venv
   source .venv/bin/activate
   pip install -r requirements.txt
   ```
````

## File: frontend/src/app/chat/ChatUI.tsx
````typescript
'use client'
import { useState } from 'react'

export default function ChatUI() {
  const [input, setInput] = useState('')
  const [messages, setMessages] = useState<Array<{role: string, content: string}>>([])
  const [isLoading, setIsLoading] = useState(false)

  const handleSubmit = async () => {
    if (!input || isLoading) return
    
    try {
      setIsLoading(true)
      const userMessage = { role: 'user', content: input }
      setMessages(prev => [...prev, userMessage])
      const backendUrl:string = process.env.NEXT_PUBLIC_API_URL || 'http://localhost:8000'
      const response = await fetch(`${backendUrl}/api/chat`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ message: input }),
      })

      if (!response.ok) throw new Error('API Error')
      
      const data = await response.json()
      setMessages(prev => [...prev, { role: 'assistant', content: data.response }])
      
    } catch (error) {
      console.error('Error:', error)
      setMessages(prev => [...prev, { 
        role: 'assistant', 
        content: 'エラーが発生しました。もう一度お試しください。'
      }])
    } finally {
      setInput('')
      setIsLoading(false)
    }
  }

  return (
    <div className="max-w-2xl mx-auto p-4">
      <div className="border rounded-lg p-4 mb-4 h-[600px] overflow-y-auto bg-gray-50">
        {messages.map((msg, i) => (
          <div key={i} className={`mb-3 ${msg.role === 'user' ? 'text-right' : ''}`}>
            <div className={`inline-block p-3 rounded-lg max-w-[80%] whitespace-pre-wrap ${
              msg.role === 'user' 
                ? 'bg-blue-500 text-white' 
                : 'bg-white text-gray-800 border'
            }`}>
              {msg.content}
            </div>
          </div>
        ))}
      </div>

      <div className="flex gap-2">
        <input
          value={input}
          onChange={(e) => setInput(e.target.value)}
          onKeyDown={(e) => e.key === 'Enter' && !e.shiftKey && handleSubmit()}
          className="flex-1 border rounded-lg px-4 py-2 focus:outline-none focus:ring-2 focus:ring-blue-500"
          placeholder="プロジェクトの要件を入力してください..."
          disabled={isLoading}
        />
        <button
          onClick={handleSubmit}
          className={`px-6 py-2 rounded-lg text-white ${
            isLoading ? 'bg-gray-400' : 'bg-blue-500 hover:bg-blue-600'
          }`}
          disabled={isLoading}
        >
          {isLoading ? '生成中...' : '送信'}
        </button>
      </div>
    </div>
  )
}
````

## File: frontend/src/app/chat/page.tsx
````typescript
import ChatUI from './ChatUI'

export default function ChatPage() {
  return (
    <div className="min-h-screen bg-gray-100 py-6">
      <div className="max-w-4xl mx-auto">
        <h1 className="text-3xl font-bold text-center mb-8 text-gray-800">
          要件定義生成チャット
        </h1>
        <ChatUI />
      </div>
    </div>
  )
}
````

## File: frontend/src/app/globals.css
````css
@tailwind base;
@tailwind components;
@tailwind utilities;
````

## File: frontend/src/app/layout.tsx
````typescript
import './globals.css'
import type { Metadata } from 'next'

export const metadata: Metadata = {
  title: 'DocuBot',
  description: 'Documentation Generator',
  icons: {
    icon: '/favicon.ico'
  }
}

export default function RootLayout({
  children,
}: {
  children: React.ReactNode
}) {
  return (
    <html lang="ja">
      <body>{children}</body>
    </html>
  )
}
````

## File: frontend/src/app/page.tsx
````typescript
import { redirect } from 'next/navigation'

export default function Home() {
  redirect('/chat')
  return null
}
````

## File: frontend/src/public/next.svg
````
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 394 80"><path fill="#000" d="M262 0h68.5v12.7h-27.2v66.6h-13.6V12.7H262V0ZM149 0v12.7H94v20.4h44.3v12.6H94v21h55v12.6H80.5V0h68.7zm34.3 0h-17.8l63.8 79.4h17.9l-32-39.7 32-39.6h-17.9l-23 28.6-23-28.6zm18.3 56.7-9-11-27.1 33.7h17.8l18.3-22.7z"/><path fill="#000" d="M81 79.3 17 0H0v79.3h13.6V17l50.2 62.3H81Zm252.6-.4c-1 0-1.8-.4-2.5-1s-1.1-1.6-1.1-2.6.3-1.8 1-2.5 1.6-1 2.6-1 1.8.3 2.5 1a3.4 3.4 0 0 1 .6 4.3 3.7 3.7 0 0 1-3 1.8zm23.2-33.5h6v23.3c0 2.1-.4 4-1.3 5.5a9.1 9.1 0 0 1-3.8 3.5c-1.6.8-3.5 1.3-5.7 1.3-2 0-3.7-.4-5.3-1s-2.8-1.8-3.7-3.2c-.9-1.3-1.4-3-1.4-5h6c.1.8.3 1.6.7 2.2s1 1.2 1.6 1.5c.7.4 1.5.5 2.4.5 1 0 1.8-.2 2.4-.6a4 4 0 0 0 1.6-1.8c.3-.8.5-1.8.5-3V45.5zm30.9 9.1a4.4 4.4 0 0 0-2-3.3 7.5 7.5 0 0 0-4.3-1.1c-1.3 0-2.4.2-3.3.5-.9.4-1.6 1-2 1.6a3.5 3.5 0 0 0-.3 4c.3.5.7.9 1.3 1.2l1.8 1 2 .5 3.2.8c1.3.3 2.5.7 3.7 1.2a13 13 0 0 1 3.2 1.8 8.1 8.1 0 0 1 3 6.5c0 2-.5 3.7-1.5 5.1a10 10 0 0 1-4.4 3.5c-1.8.8-4.1 1.2-6.8 1.2-2.6 0-4.9-.4-6.8-1.2-2-.8-3.4-2-4.5-3.5a10 10 0 0 1-1.7-5.6h6a5 5 0 0 0 3.5 4.6c1 .4 2.2.6 3.4.6 1.3 0 2.5-.2 3.5-.6 1-.4 1.8-1 2.4-1.7a4 4 0 0 0 .8-2.4c0-.9-.2-1.6-.7-2.2a11 11 0 0 0-2.1-1.4l-3.2-1-3.8-1c-2.8-.7-5-1.7-6.6-3.2a7.2 7.2 0 0 1-2.4-5.7 8 8 0 0 1 1.7-5 10 10 0 0 1 4.3-3.5c2-.8 4-1.2 6.4-1.2 2.3 0 4.4.4 6.2 1.2 1.8.8 3.2 2 4.3 3.4 1 1.4 1.5 3 1.5 5h-5.8z"/></svg>
````

## File: frontend/src/public/vercel.svg
````
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 283 64"><path fill="black" d="M141 16c-11 0-19 7-19 18s9 18 20 18c7 0 13-3 16-7l-7-5c-2 3-6 4-9 4-5 0-9-3-10-7h28v-3c0-11-8-18-19-18zm-9 15c1-4 4-7 9-7s8 3 9 7h-18zm117-15c-11 0-19 7-19 18s9 18 20 18c6 0 12-3 16-7l-8-5c-2 3-5 4-8 4-5 0-9-3-11-7h28l1-3c0-11-8-18-19-18zm-10 15c2-4 5-7 10-7s8 3 9 7h-19zm-39 3c0 6 4 10 10 10 4 0 7-2 9-5l8 5c-3 5-9 8-17 8-11 0-19-7-19-18s8-18 19-18c8 0 14 3 17 8l-8 5c-2-3-5-5-9-5-6 0-10 4-10 10zm83-29v46h-9V5h9zM37 0l37 64H0L37 0zm92 5-27 48L74 5h10l18 30 17-30h10zm59 12v10l-3-1c-6 0-10 4-10 10v15h-9V17h9v9c0-5 6-9 13-9z"/></svg>
````

## File: frontend/.eslintrc.json
````json
{
  "extends": "next/core-web-vitals"
}
````

## File: frontend/Dockerfile
````
# syntax=docker/dockerfile:1
FROM node:20-slim as base

WORKDIR /app

# 開発環境用
FROM base as init
COPY package*.json ./
RUN npm install
EXPOSE 3000
CMD ["npm", "run", "dev"]

# 本番環境用
FROM base as runner
COPY . .
RUN npm ci
RUN npm run build
EXPOSE 3000
CMD ["npm", "start"]
````

## File: frontend/next.config.js
````javascript
/** @type {import('next').NextConfig} */
const nextConfig = {
  output: "standalone",
  async rewrites() {
    return [
      {
        source: "/api/:path*",
        destination:
          process.env.NODE_ENV === "development"
            ? "http://127.0.0.1:8000/api/:path*"
            : "/api/:path*",
      },
    ];
  },
};

module.exports = nextConfig;
````

## File: frontend/package.json
````json
{
  "name": "nextjs-fastapi",
  "version": "0.2.0",
  "private": true,
  "scripts": {
    "fastapi-dev": "uv pip install -e . && python3 -m uvicorn api.main:app --reload",
    "next-dev": "next dev",
    "dev": "concurrently \"npm run next-dev\" \"npm run fastapi-dev\"",
    "build": "next build",
    "start": "next start",
    "lint": "next lint"
  },
  "dependencies": {
    "@types/node": "22.5.5",
    "@types/react": "18.3.8",
    "@types/react-dom": "18.3.0",
    "autoprefixer": "^10.4.20",
    "concurrently": "^9.0.1",
    "eslint": "8.41.0",
    "eslint-config-next": "13.4.4",
    "next": "^14.2.13",
    "postcss": "^8.4.35",
    "react": "^18.2.0",
    "react-dom": "^18.2.0",
    "tailwindcss": "3.4.12",
    "typescript": "5.6.2"
  }
}
````

## File: frontend/postcss.config.js
````javascript
module.exports = {
  plugins: {
    tailwindcss: {},
    autoprefixer: {},
  },
}
````

## File: frontend/tailwind.config.js
````javascript
/** @type {import('tailwindcss').Config} */
module.exports = {
  content: [
    './app/**/*.{js,ts,jsx,tsx,mdx}',
    './components/**/*.{js,ts,jsx,tsx,mdx}',
    '!./node_modules/**/*'
  ],
  theme: {
    extend: {
      backgroundImage: {
        'gradient-radial': 'radial-gradient(var(--tw-gradient-stops))',
        'gradient-conic':
          'conic-gradient(from 180deg at 50% 50%, var(--tw-gradient-stops))',
      },
    },
  },
  plugins: [],
}
````

## File: frontend/tsconfig.json
````json
{
  "compilerOptions": {
    "target": "es5",
    "lib": ["dom", "dom.iterable", "esnext"],
    "allowJs": true,
    "skipLibCheck": true,
    "strict": true,
    "forceConsistentCasingInFileNames": true,
    "noEmit": true,
    "esModuleInterop": true,
    "module": "esnext",
    "moduleResolution": "node",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "preserve",
    "incremental": true,
    "plugins": [
      {
        "name": "next"
      }
    ],
    "paths": {
      "@/*": ["./*"]
    }
  },
  "include": ["next-env.d.ts", "**/*.ts", "**/*.tsx", ".next/types/**/*.ts"],
  "exclude": ["node_modules"]
}
````

## File: terraform/app/main.tf
````hcl
terraform {
  required_version = "~> 1.5.0"
  required_providers {
    google = {
      source  = "hashicorp/google"
      version = "~> 5.0"
    }
  }
  backend "gcs" {}
}

provider "google" {
  project = var.project_id
  region  = var.region
}

# Backend Service
resource "google_cloud_run_service" "backend" {
  name     = "backend"
  location = var.region

  template {
    spec {
      containers {
        image = var.backend_image
        ports {
          container_port = 8080
        }
        env {
          name  = "PORT"
          value = "8080"
        }
      }
    }
  }
}

# Frontend Service
resource "google_cloud_run_service" "frontend" {
  name     = "frontend"
  location = var.region

  template {
    spec {
      containers {
        image = var.frontend_image
        ports {
          container_port = 3000
        }
        env {
          name  = "NEXT_PUBLIC_API_URL"
          value = google_cloud_run_service.backend.status[0].url
        }
      }
    }
  }
}

# IAM
data "google_iam_policy" "noauth" {
  binding {
    role = "roles/run.invoker"
    members = [
      "allUsers",
    ]
  }
}

resource "google_cloud_run_service_iam_policy" "backend_noauth" {
  location = google_cloud_run_service.backend.location
  project  = google_cloud_run_service.backend.project
  service  = google_cloud_run_service.backend.name

  policy_data = data.google_iam_policy.noauth.policy_data
}

resource "google_cloud_run_service_iam_policy" "frontend_noauth" {
  location = google_cloud_run_service.frontend.location
  project  = google_cloud_run_service.frontend.project
  service  = google_cloud_run_service.frontend.name

  policy_data = data.google_iam_policy.noauth.policy_data
}

# Output
output "backend_url" {
  value = google_cloud_run_service.backend.status[0].url
}

output "frontend_url" {
  value = google_cloud_run_service.frontend.status[0].url
}
````

## File: terraform/app/variables.tf
````hcl
variable "project_id" {
  type = string
}

variable "region" {
  type = string
}

variable "backend_image" {
  type = string
}

variable "frontend_image" {
  type = string
}
````

## File: terraform/init/main.tf
````hcl
terraform {
  required_version = "~> 1.5.0"
  required_providers {
    google = {
      source  = "hashicorp/google"
      version = "~> 5.0"
    }
  }
}

provider "google" {
  project = var.project_id
}

# Artifact Registry
resource "google_artifact_registry_repository" "repo" {
  location      = var.location
  repository_id = var.artifact_registry_repository_id
  format        = "DOCKER"
}

# Service Account for Operation
resource "google_service_account" "operation_account" {
  account_id   = var.operation_sa_id
  display_name = var.operation_sa_display_name
}

# Service Account for Build
resource "google_service_account" "build_account" {
  account_id   = var.build_sa_id
  display_name = var.build_sa_display_name
}

# IAM
resource "google_project_iam_member" "operation_account_roles" {
  for_each = toset([
    "roles/run.admin",
    "roles/artifactregistry.reader"
  ])
  project = var.project_id
  role    = each.key
  member  = "serviceAccount:${google_service_account.operation_account.email}"
}

resource "google_project_iam_member" "build_account_roles" {
  for_each = toset([
    "roles/run.developer",
    "roles/artifactregistry.writer"
  ])
  project = var.project_id
  role    = each.key
  member  = "serviceAccount:${google_service_account.build_account.email}"
}

# Workload Identity Pool
resource "google_iam_workload_identity_pool" "github_pool" {
  workload_identity_pool_id = var.workload_identity_pool_id
}

resource "google_iam_workload_identity_pool_provider" "github_provider" {
  workload_identity_pool_id          = google_iam_workload_identity_pool.github_pool.workload_identity_pool_id
  workload_identity_pool_provider_id = var.workload_identity_provider_id
  attribute_mapping = {
    "google.subject"       = "assertion.sub"
    "attribute.actor"      = "assertion.actor"
    "attribute.repository" = "assertion.repository"
  }
  oidc {
    issuer_uri = "https://token.actions.githubusercontent.com"
  }
}

resource "google_service_account_iam_member" "pool_impersonation" {
  service_account_id = google_service_account.build_account.name
  role               = "roles/iam.workloadIdentityUser"
  member             = "principalSet://iam.googleapis.com/${google_iam_workload_identity_pool.github_pool.name}/attribute.repository/${var.github_repo_owner}/${var.github_repo_name}"
}

# Output
output "build_service_account_email" {
  value = google_service_account.build_account.email
}

output "operation_service_account_email" {
  value = google_service_account.operation_account.email
}

output "workload_identity_provider_name" {
  value = "${google_iam_workload_identity_pool.github_pool.name}/providers/${google_iam_workload_identity_pool_provider.github_provider.workload_identity_pool_provider_id}"
}
````

## File: terraform/init/variables.tf
````hcl
variable "project_id" {
  type = string
}

variable "location" {
  type = string
}

variable "operation_sa_id" {
  type = string
}

variable "operation_sa_display_name" {
  type = string
}

variable "build_sa_id" {
  type = string
}

variable "build_sa_display_name" {
  type = string
}

variable "artifact_registry_repository_id" {
  type = string
}

variable "github_repo_owner" {
  type = string
}

variable "github_repo_name" {
  type = string
}

variable "workload_identity_pool_id" {
  type = string
}

variable "workload_identity_provider_id" {
  type = string
}
````

## File: .env_sample
````
# Frontend port (Next.js)
FRONTEND_PORT=3000

# Backend port (FastAPI)
BACKEND_PORT=8080
````

## File: .gitignore
````
# See https://help.github.com/articles/ignoring-files/ for more about ignoring files.

# dependencies
/node_modules
/.pnp
.pnp.js

# testing
/coverage

# next.js
/.next/
/out/

# production
/build

# misc
.DS_Store
*.pem

# debug
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# local env files
.env*.local

# vercel
.vercel

# typescript
*.tsbuildinfo
next-env.d.ts

# python
__pycache__/
venv

# environment variables
.env
.env.local
# Next.js
/app/.next/
/app/node_modules/
````

## File: .windsurfrules
````
あなたは高度な問題解決能力を持つAIアシスタント、Windsurf Cascadeです。以下の指示に従って、効率的かつ正確にタスクを遂行してください。

# 基本動作原則

1. **指示の受信と理解**
   - ユーザーからの指示を注意深く読み取り
   - 不明点がある場合は、具体的な質問を行う
   - 技術的な制約や要件を明確に把握

2. **深い分析とプランニング**
   ```markdown
   ## タスク分析
   - 目的：[タスクの最終目標]
   - 技術要件：[必要な技術要素]
   - 実装手順：[具体的なステップ]
   - リスク：[潜在的な問題点]
   - 品質基準：[満たすべき基準]
   ```

3. **実装計画の策定**
   ```markdown
   ## 実装計画
   1. [具体的なステップ1]
      - 詳細な実装内容
      - 予想される課題と対策
   2. [具体的なステップ2]
      ...
   ```

4. **段階的な実装と検証**
   - 各ステップの完了後に検証
   - 問題発生時の即時対応
   - 品質基準との照合

5. **継続的なフィードバック**
   - 実装の進捗状況を定期的に報告
   - 重要な判断ポイントでの確認
   - 問題発生時の迅速な報告

---

# 品質管理プロトコル

## 1. コード品質
- 言語・フレームワークの標準規約準拠
- コーディング規約の一貫性維持
- 適切なコメント・ドキュメント化

## 2. パフォーマンス
- リソース使用の最適化
- 処理速度の効率化
- スケーラビリティの考慮

## 3. セキュリティ
- 入力値の厳格なバリデーション
- 適切なエラーハンドリング
- 機密情報の安全な管理

## 4. 保守性
- コードの可読性維持
- モジュール化の推進
- テストの容易性確保

---

# 実装プロセス

## 1. 初期分析フェーズ
```markdown
### 要件分析
- 機能要件の特定
- 技術的制約の確認
- 既存コードとの整合性確認

### リスク評価
- 潜在的な技術的課題
- パフォーマンスへの影響
- セキュリティリスク
```

## 2. 実装フェーズ
- 段階的な実装
- 各段階での検証
- コード品質の維持

## 3. 検証フェーズ
- 単体テスト
- 統合テスト
- パフォーマンステスト

## 4. 最終確認
- 要件との整合性
- コード品質
- ドキュメント完成度

---

# エラー対応プロトコル

1. **問題の特定**
   - エラーメッセージの解析
   - 影響範囲の特定
   - 原因の切り分け

2. **解決策の策定**
   - 複数の対応案の検討
   - リスク評価
   - 最適解の選択

3. **実装と検証**
   - 解決策の実装
   - テストによる検証
   - 副作用の確認

4. **文書化**
   - 問題と解決策の記録
   - 再発防止策の提案
   - 学習点の共有

---

# バージョン管理プロトコル

1. **バージョン管理の基本原則**
   - 変更履歴の明確な記録
   - 依存関係の整合性維持
   - バージョン番号の適切な更新

2. **変更管理プロセス**
   - 変更の影響範囲の評価
   - 互換性の確認
   - 段階的な更新の実施

3. **リリース管理**
   - リリースノートの作成
   - 変更点の明確な文書化
   - ロールバック手順の準備

---

以上の指示に従い、確実で質の高い実装を行います。不明点や重要な判断が必要な場合は、必ず確認を取ります。
````

## File: compose-init.yaml
````yaml
services:
  frontend:
    build:
      context: ./frontend
      target: init
    tty: true
    stdin_open: true
    init: true
    volumes:
      - ./frontend/src:/app

  backend:
    build:
      context: ./backend
      target: init
    tty: true
    stdin_open: true
    init: true
    volumes:
      - ./backend/src:/app
````

## File: compose.yaml
````yaml
services:
  frontend:
    build:
      context: ./frontend
      target: runner
    ports:
      - "${FRONTEND_PORT:-3000}:3000"
    environment:
      - NEXT_PUBLIC_API_URL=http://backend:${BACKEND_PORT:-8080}
    tty: true
    stdin_open: true
    init: true
    volumes:
      - ./frontend/src:/app
    depends_on:
      - backend

  backend:
    build:
      context: ./backend
      target: runner
    ports:
      - "${BACKEND_PORT:-8080}:8080"
    tty: true
    stdin_open: true
    init: true
    environment:
      - PORT=${BACKEND_PORT:-8080}
    volumes:
      - ./backend/src:/app
    command: go run .
````

## File: Makefile
````
.PHONY: build
build:
	docker compose build --no-cache

.PHONY: up
up:
	docker compose up -d

.PHONY: stop
stop:
	docker compose stop

.PHONY: down
down:
	docker compose down

.PHONY: init-start
init-start:
	@make init
	@make start

.PHONY: init
init:
	@make build
	@make front-install

.PHONY: dev
dev:
	@make up
	@make front-install
	@make web-npm-dev

.PHONY: start
start:
	@make web-npm-build
	@make up
	@make web-npm-start

.PHONY: front-init
front-init:
	@make front-build
	@make front-install
	@make front-up

.PHONY: front-restart
front-restart:
	@make front-down
	@make front-up

.PHONY: front-build
front-build:
	docker compose build --no-cache frontend

.PHONY: front-build-with-cache
front-build-with-cache:
	docker compose build frontend

.PHONY: front-up
front-up:
	docker compose up -d frontend

.PHONY: front-stop
front-stop:
	docker compose stop frontend

.PHONY: front-down
front-down:
	docker compose down frontend

.PHONY: npm-install
npm-install:
	docker compose run --rm frontend npm install

.PHONY: front-cache-check
front-cache-check:
	docker compose run --rm frontend yarn check --verify-tree

.PHONY: web-yarn-build
web-yarn-build:
	docker compose run --rm frontend yarn build

.PHONY: web-yarn-start
web-yarn-start:
	docker compose exec frontend yarn start

.PHONY: web-npm-dev
web-npm-dev:
	docker compose exec frontend npm run dev

.PHONY: web-npm-build
web-npm-build:
	docker compose run --rm frontend npm run build

.PHONY: web-npm-start
 web-npm-start:
	docker compose exec frontend npm run start

.PHONY: web-serverjs
web-serverjs:
	docker compose exec frontend node .next/standalone/server.js

.PHONY: back-init
back-init:
	@make back-build
	@make back-up

.PHONY: back-restart
back-restart:
	@make back-down
	@make back-up

.PHONY: back-build
back-build:
	docker compose build --no-cache backend

.PHONY: back-build-with-cache
back-build-with-cache:
	docker compose build backend

.PHONY: go-mod-tidy
go-mod-tidy:
	docker compose run --rm backend go mod tidy

.PHONY: back-up
back-up:
	docker compose up -d backend

.PHONY: back-stop
back-stop:
	docker compose stop backend

.PHONY: back-down
back-down:
	docker compose down backend

include .env
NETWORK_NAME=network-prod

.PHONY: prod-init-start
prod-init-start:
	@make prod-init
	@make prod-start

.PHONY: prod-init
prod-init:
	@make prod-network-create
	@make prod-back-build
	@make prod-front-build

.PHONY: prod-init-with-cache
prod-init-with-cache:
	@make prod-network-create
	@make prod-back-build-with-cache
	@make prod-front-build-with-cache

.PHONY: prod-start
prod-start:
	@make prod-back-run
	@make prod-front-run

.PHONY: prod-stop
prod-stop:
	@make prod-back-stop
	@make prod-front-stop

.PHONY: prod-down
prod-down:
	@make prod-back-stop
	@make prod-front-stop
	@make prod-back-down
	@make prod-front-down
	@make prod-network-remove

.PHONY: prod-back-build
prod-back-build:
	docker build --no-cache -t backend-prod --build-arg API_PORT=${BACKEND_PORT} ./backend
.PHONY: prod-back-build-with-cache
prod-back-build-with-cache:
	docker build -t backend-prod --build-arg API_PORT=${BACKEND_PORT} ./backend
.PHONY: prod-back-run
prod-back-run:
	docker run --rm --name backend-prod -d -p ${BACKEND_PORT}:8080 --network $(NETWORK_NAME) backend-prod
.PHONY: prod-back-stop
prod-back-stop:
	docker stop backend-prod
.PHONY: prod-back-down
prod-back-down:
	docker rmi -f backend-prod

.PHONY: prod-front-build
prod-front-build:
	docker build --no-cache -t frontend-prod --build-arg NEXT_PUBLIC_API_URL=http://backend-prod:8081 ./frontend
.PHONY: prod-front-build-with-cache
prod-front-build-with-cache:
	docker build -t frontend-prod --build-arg NEXT_PUBLIC_API_URL=http://backend-prod:8081 ./frontend
.PHONY: prod-front-run
prod-front-run:
	docker run --rm --init --name frontend-prod -d -p ${FRONTEND_PORT}:3000 --network $(NETWORK_NAME) frontend-prod
.PHONY: prod-front-stop
prod-front-stop:
	docker stop frontend-prod
.PHONY: prod-front-down
prod-front-down:
	docker rmi -f frontend-prod 


.PHONY: prod-network-create
prod-network-create:
	@if [ -z "$$(docker network ls --filter name=^$(NETWORK_NAME)$$ --format='{{ .Name }}')" ]; then \
		docker network create --driver=bridge $(NETWORK_NAME); \
		echo "$(NETWORK_NAME) created"; \
	else \
		echo "$(NETWORK_NAME) already exists"; \
	fi
.PHONY: prod-network-remove
prod-network-remove:
	docker network rm $(NETWORK_NAME)

.PHONY: ps
ps:
	docker compose ps

.PHONY: python-install
python-install:
	docker compose -f compose-init.yaml run --rm backend pip install -r requirements.txt

.PHONY: clean
clean:
	docker compose down --rmi all --volumes --remove-orphans
````

## File: README.md
````markdown
# DocumentBot Agent

<p align="center">
  <h3 align="center">Next.js + FastAPI Documentation Agent</h3>
</p>

<p align="center">AIを活用したドキュメンテーションアシスタント。Next.js 14とFastAPIを使用したモダンなウェブアプリケーション。</p>

<br/>

## 概要

このプロジェクトは、OpenAIのAPIを活用してドキュメンテーションの作成・管理を支援するAIアシスタントです。フロントエンドにNext.js 14、バックエンドにFastAPIを採用し、モダンで使いやすいインターフェースを提供します。

## 主な機能

- 💬 AIを活用したドキュメント作成支援
- 🎯 コンテキストを考慮した的確な応答
- 🌐 モダンなウェブインターフェース
- ⚡ 高速なレスポンス

## 技術スタック

- フロントエンド: Next.js 14, TailwindCSS
- バックエンド: FastAPI, LangChain
- AI: OpenAI API

## 開発環境のセットアップ

1. 環境変数の設定:

```bash
# .envファイルを作成
cp .env.sample .env
# OpenAI APIキーを設定
```

2. Python仮想環境の作成と有効化:

```bash
python3 -m venv venv
source venv/bin/activate
```

3. 依存関係のインストール:

```bash
# フロントエンド
npm install

# バックエンド
pip install -e .
```

4. 開発サーバーの起動:

```bash
npm run dev
```

アプリケーションは以下のURLで起動します：
- フロントエンド: http://localhost:3000
- バックエンドAPI: http://localhost:8000

## APIエンドポイント

- `POST /api/chat`: チャットメッセージの送受信
- `GET /health`: ヘルスチェック

## 環境変数

- `OPENAI_API_KEY`: OpenAI APIキー（必須）
- `PORT`: バックエンドサーバーのポート番号（デフォルト: 8000）

## ライセンス

このプロジェクトはMITライセンスの下で公開されています。

# Cloud RunのCI/CDをGithub Actionsで実装するサンプル

* Cloud Runプロジェクト用のCI/CD構築
* Github ActionsでCI/CDワークフロー管理
* Terraformでデプロイ管理
* フロントエンドはNext.js、バックエンドはPython (FastAPI)

## 動作環境

### ローカル環境
* Docker: 4.x
* Terraform: v1.5.x
* GNU Make: 3.x
* VS Code

### プロジェクト
* Python: 3.10
* Node.js: 20
* Next.js: 14

### Google Cloud
* Artifact Registry
* Cloud Run
* Cloud Storage

## ローカル環境（dev stage）

docker composeを利用してdev stageでの動作確認方法を示します。

### 初期設定

#### 環境変数設定
1. `.env_sample`ファイルを複製して`.env`にリネーム
2. 各変数を設定。※ローカル環境で利用する情報です。
| 変数名 | 説明 | 例 |
|---|---|---|
| BACKEND_PORT | バックエンドポート（任意） | 8080 |
| FRONTEND_PORT | フロントエンドポート（任意） | 3000 |

### サービス起動&停止

1. makeコマンドで起動
```bash
make up
```

2. makeコマンドで停止
```bash
make down
```

### 起動方法
1. makeコマンド`make start`で起動する
2. http://localhost:<ポート番号>にアクセス ※デフォルトは http://localhost:3000

## 🚀本番環境🚀 Google Cloud, Cloud Run

### デプロイ方法

1. Google Cloudのプロジェクトを新規作成する。※Google Cloudのコンソールで操作

2. Google Cloudのリソース作成 ※ローカルPCのコマンドラインから操作
   1. `terraform/init/terraform.tfvars_sample`を複製して`terraform/init/terraform.tfvars`にリネーム
   2. 各種変数を設定する
   | 変数名 | 説明 |
   |---|---|
   | project_id | Google CloudのプロジェクトID |
   | location | サービスをデプロイするlocation |
   | operation_sa_id | サービス運用アカウントID |
   | operation_sa_display_name | サービス運用アカウント表示名 |
   | build_sa_id | ビルドアカウントID |
   | build_sa_display_name | ビルドアカウント表示名 |
   | artifact_registry_repository_id | Artifact RegistryのリポジトリID |
   | github_repo_owner | githubのリポジトリオーナ名 |
   | github_repo_name | githubのリポジトリ名 |
   | workload_identity_pool_id | Worklaod Identity Pool ID |
   | workload_identity_provider_id | Worklaod Identity Provider ID |

   3. cdコマンドで`terraform/init`に移動
   4. デプロイする
   ```bash
   terraform fmt
   terraform init
   terraform validate
   terraform plan
   terraform apply
   ```
   5. terraform applyの後に表示される3つの変数は手順4で利用します。
   ```
   build_service_account_email = "***@****.iam.gserviceaccount.com"
   operation_service_account_email = "***@****.iam.gserviceaccount.com"
   workload_identity_provider_name = "projects/***/locations/global/workloadIdentityPools/***/providers/***"
   ```

3. application用terraform.tfstate保存用バケットを作成
   1. `terraform/bucket/terraform.tfvars_sample`を複製して`terraform/bucket/terraform.tfvars`にリネーム
   2. `terraform.tfvars`のproject_idを設定する。
   3. cdコマンドで`terraform/bucket`に移動
   4. デプロイする
   ```bash
   terraform fmt
   terraform init
   terraform validate
   terraform plan
   terraform apply
   ```

4. Githubにシークレットを設定
   1. Githubのリポジトリにアクセスし、Setting>Secrets and Variables>Actionsで下表の変数を設定する。※Github Actionsで利用する変数
   | 変数名 | 説明 |
   |---|---|
   | GCP_PROJECT_ID | プロジェクトID |
   | GCP_REGION | リージョン（location） |
   | ARTIFACT_REPO | Artifact Registoryのリポジトリ名 |
   | BUILD_ACCOUNT | ビルドアカウントのID |
   | OPERATION_ACCOUNT | 運用アカウントのID |
   | WORKLOAD_IDENTITY_PROVIDER | WORKLOAD_IDENTITY_PROVIDERのID |

5. Github Actionsを走らせてCloud Runにデプロイする。
   1. 適当なブランチを作成してプッシュ
   2. Githubでmainブランチへのプルリクエストを出す
   3. Actionsが実行されてArtifact RegistoryにDockerイメージがデプロイされ、terraform planが終わるまで待機
   4. terraform planの結果を見てデプロイして問題ないか確認
   5. プルリクエストをマージ
   6. Actionsが実行されてCloud Runにデプロイされる

## 💀Google Cloudリソース削除方法💀

### 手順1. Cloud Runを削除する ※Github Actionsから操作
1. GithubのActionsタブにある`terraform-destroy`workflowを押してRun workflowを実行
2. Actionsが実行されてCloud Runリソースが削除されたことを確認

### 手順2. バケットを削除 ※ローカルPCのターミナルで操作
1. cdコマンドで`terraform/init`に移動
2. `terraform plan --lock=false -destroy`を実行してapply内容を確認
3. `terraform apply -destroy`を実行して削除されたことを確認

### 手順3. バケットを削除 ※Google Cloudのコンソールから操作
1. Google Cloudのコンソールにアクセス
2. Cloud Storageのバケットタブに移動
3. 対象のバケットを選択して削除

### 手順4. プロジェクトを削除 ※Google Cloudのコンソールから操作
1. Google Cloudのコンソールにアクセス
2. 右上の︙を押して「プロジェクトの設定」に移動
3. シャットダウンを押す
4. ダイアログにプロジェクトIDを入力して「このままシャットダウン」を押す
````
